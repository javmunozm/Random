using System;
using System.Collections.Generic;
using System.Linq;
using DataProcessor.Connections;

namespace DataProcessor.Models
{
    public class RetrainedEnhancedModel
    {
        private readonly ImprovedFrequencyLearning frequencyLearner;
        private readonly DatabaseConnection dbConnection;
        private Dictionary<int, double> retrainedWeights;
        private Dictionary<int, double> validationPerformance;
        private readonly Random random;

        public RetrainedEnhancedModel()
        {
            frequencyLearner = new ImprovedFrequencyLearning();
            dbConnection = new DatabaseConnection();
            retrainedWeights = new Dictionary<int, double>();
            validationPerformance = new Dictionary<int, double>();
            random = new Random(42);

            InitializeRetrainedWeights();
        }

        private void InitializeRetrainedWeights()
        {
            // Initialize with enhanced 3126 corrections
            for (int i = 1; i <= 25; i++)
            {
                retrainedWeights[i] = 1.0;
                validationPerformance[i] = 0.0;
            }

            // Apply all known corrections from validation results
            ApplyValidationBasedCorrections();
        }

        private void ApplyValidationBasedCorrections()
        {
            // Based on comprehensive validation, apply additional corrections

            // Numbers that performed well across multiple series - boost more
            retrainedWeights[2] = 4.0;   // Consistently high performance
            retrainedWeights[3] = 3.2;   // Good cross-series performance
            retrainedWeights[9] = 3.0;   // Strong in multiple series
            retrainedWeights[10] = 2.8;  // Consistent performer
            retrainedWeights[11] = 2.8;  // High frequency across series
            retrainedWeights[12] = 3.5;  // Very strong performer
            retrainedWeights[13] = 2.5;  // Good consistency
            retrainedWeights[19] = 2.2;  // Emerging pattern
            retrainedWeights[20] = 2.5;  // Strong cross-series
            retrainedWeights[21] = 2.3;  // Good performance
            retrainedWeights[22] = 2.8;  // Very consistent
            retrainedWeights[23] = 3.0;  // High performance
            retrainedWeights[24] = 2.7;  // Good consistency

            // Numbers that underperformed - reduce slightly
            retrainedWeights[4] = 0.9;
            retrainedWeights[6] = 0.8;
            retrainedWeights[7] = 0.85;
            retrainedWeights[8] = 0.9;
            retrainedWeights[14] = 0.7;  // Over-predicted in original
            retrainedWeights[16] = 0.75;
            retrainedWeights[17] = 0.8;
            retrainedWeights[18] = 0.7;  // Over-predicted in original

            // Special corrections based on patterns
            retrainedWeights[1] = 2.0;   // Lower range boost
            retrainedWeights[5] = 2.1;   // Mid-low range important
            retrainedWeights[15] = 3.5;  // Critical miss in 3126 - major boost
            retrainedWeights[25] = 1.8;  // Upper range moderate boost
        }

        public void RetrainWithAllCurrentData()
        {
            Console.WriteLine("ðŸ”„ Retraining enhanced model with all current data...");

            // Load all available data up to the latest series
            var allHistoricalData = dbConnection.LoadHistoricalDataBefore(3127);
            Console.WriteLine($"ðŸ“Š Training with {allHistoricalData.Count} complete series");

            // Update frequency learning with all data
            frequencyLearner.UpdateWithHistoricalData();

            // Analyze recent performance patterns (last 10 series)
            AnalyzeRecentPerformancePatterns(allHistoricalData);

            // Update weights based on comprehensive analysis
            UpdateWeightsFromComprehensiveAnalysis(allHistoricalData);

            Console.WriteLine("âœ… Retrained model with comprehensive data analysis");
        }

        private void AnalyzeRecentPerformancePatterns(List<DataProcessor.Connections.SeriesData> historicalData)
        {
            var recentData = historicalData.TakeLast(10).ToList();
            var recentFrequencies = new Dictionary<int, int>();

            for (int i = 1; i <= 25; i++)
            {
                recentFrequencies[i] = 0;
            }

            // Count frequencies in recent data with higher weight
            foreach (var series in recentData)
            {
                foreach (var combination in series.AllCombinations)
                {
                    foreach (int number in combination)
                    {
                        recentFrequencies[number] += 2; // Double weight for recent patterns
                    }
                }
            }

            // Apply recent pattern adjustments
            int totalRecentCount = recentFrequencies.Values.Sum();
            foreach (var kvp in recentFrequencies)
            {
                double recentFrequency = (double)kvp.Value / totalRecentCount;

                // Boost numbers with high recent frequency
                if (recentFrequency > 0.15) // 15% threshold for recent importance
                {
                    retrainedWeights[kvp.Key] = Math.Min(5.0, retrainedWeights[kvp.Key] * 1.3);
                }
            }
        }

        private void UpdateWeightsFromComprehensiveAnalysis(List<DataProcessor.Connections.SeriesData> historicalData)
        {
            // Analyze cross-series patterns
            var crossSeriesFrequencies = new Dictionary<int, List<int>>();

            for (int i = 1; i <= 25; i++)
            {
                crossSeriesFrequencies[i] = new List<int>();
            }

            // Count appearances per series
            foreach (var series in historicalData)
            {
                var seriesNumbers = new HashSet<int>();
                foreach (var combination in series.AllCombinations)
                {
                    foreach (int number in combination)
                    {
                        seriesNumbers.Add(number);
                    }
                }

                foreach (int number in seriesNumbers)
                {
                    crossSeriesFrequencies[number].Add(1);
                }
            }

            // Boost numbers that appear consistently across multiple series
            foreach (var kvp in crossSeriesFrequencies)
            {
                double consistencyRate = (double)kvp.Value.Count / historicalData.Count;

                if (consistencyRate > 0.8) // Appears in 80%+ of series
                {
                    retrainedWeights[kvp.Key] = Math.Min(6.0, retrainedWeights[kvp.Key] * 1.4);
                }
                else if (consistencyRate > 0.6) // Appears in 60%+ of series
                {
                    retrainedWeights[kvp.Key] = Math.Min(4.0, retrainedWeights[kvp.Key] * 1.2);
                }
            }
        }

        public List<int> PredictBestCombination(int targetSeriesId)
        {
            // Use improved frequency learning as base
            var baseNumbers = frequencyLearner.GenerateImprovedPrediction(targetSeriesId, 20); // Get more candidates

            // Apply retrained weights to select best 14
            var weightedNumbers = new List<(int number, double weight)>();

            foreach (int number in baseNumbers)
            {
                double combinedWeight = frequencyLearner.GetCombinedWeight(number) * retrainedWeights.GetValueOrDefault(number, 1.0);
                weightedNumbers.Add((number, combinedWeight));
            }

            // Add any high-weight numbers that might have been missed
            for (int i = 1; i <= 25; i++)
            {
                if (!baseNumbers.Contains(i) && retrainedWeights.GetValueOrDefault(i, 1.0) > 3.0)
                {
                    double weight = frequencyLearner.GetCombinedWeight(i) * retrainedWeights[i];
                    weightedNumbers.Add((i, weight));
                }
            }

            // Sort by weight and select top 14
            var selectedNumbers = weightedNumbers
                .OrderByDescending(x => x.weight)
                .Take(14)
                .Select(x => x.number)
                .OrderBy(x => x)
                .ToList();

            // Ensure we have exactly 14 numbers
            while (selectedNumbers.Count < 14)
            {
                var candidates = Enumerable.Range(1, 25).Where(n => !selectedNumbers.Contains(n));
                if (candidates.Any())
                {
                    selectedNumbers.Add(candidates.First());
                    selectedNumbers = selectedNumbers.OrderBy(x => x).ToList();
                }
                else break;
            }

            return selectedNumbers.Take(14).ToList();
        }

        public void ValidateAndLearn(int seriesId, List<int> prediction, List<List<int>> actualResults)
        {
            // Enhanced learning from actual results
            foreach (var actualCombination in actualResults)
            {
                // Standard frequency learning
                frequencyLearner.LearnFromActualResults(seriesId, actualCombination);

                // Update retrained weights based on performance
                UpdateRetrainedWeights(prediction, actualCombination);
            }

            Console.WriteLine($"ðŸ“ˆ Retrained model learning completed for series {seriesId}");
        }

        private void UpdateRetrainedWeights(List<int> prediction, List<int> actualCombination)
        {
            // Boost weights for numbers that appeared in actual but were missed in prediction
            var missed = actualCombination.Except(prediction).ToList();
            var correctPredictions = prediction.Intersect(actualCombination).ToList();
            var incorrectPredictions = prediction.Except(actualCombination).ToList();

            foreach (int number in missed)
            {
                retrainedWeights[number] = Math.Min(8.0, retrainedWeights[number] * 1.15);
            }

            foreach (int number in correctPredictions)
            {
                retrainedWeights[number] = Math.Min(6.0, retrainedWeights[number] * 1.05);
            }

            foreach (int number in incorrectPredictions)
            {
                retrainedWeights[number] = Math.Max(0.1, retrainedWeights[number] * 0.95);
            }
        }

        public Dictionary<string, object> GetRetrainingAnalysis()
        {
            var topWeights = retrainedWeights.OrderByDescending(x => x.Value).Take(15).ToDictionary(x => x.Key, x => x.Value);

            return new Dictionary<string, object>
            {
                ["TopRetrainedWeights"] = topWeights,
                ["TotalSeriesAnalyzed"] = dbConnection.LoadHistoricalDataBefore(3127).Count,
                ["ModelType"] = "Retrained Enhanced Model with Comprehensive Analysis"
            };
        }
    }
}